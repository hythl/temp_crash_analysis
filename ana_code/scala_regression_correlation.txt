import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}
import org.apache.spark.ml.regression.LinearRegression
var df = spark.read.csv("hdfs://horton.hpc.nyu.edu:8020/user/ht1128/project/new_nyc_temp_crash_edit.csv")
df = df.withColumnRenamed("_C0","day")
df = df.withColumnRenamed("_C1","time")
df = df.withColumnRenamed("_C2","temp")
df = df.withColumnRenamed("_C3","crash")
var df2 = df.select("temp","crash")
df2 = df2.withColumn("temp",col("temp").cast("Double"))
df2 = df2.withColumn("crash",col("crash").cast("Int"))
var assembler = new VectorAssembler().setInputCols(Array("temp")).setOutputCol("feature")
var df_new = assembler.transform(df2)
var lr = new LinearRegression()
lr = lr.setFeaturesCol("feature")
lr = lr.setLabelCol("crash")
var Array(train, test) = df_new.randomSplit(Array(.8,.2),42)
lr.setRegParam(0.3)
var lrModel = lr.fit(train)
val trainingSummary = lrModel.summary
print(trainingSummary.rootMeanSquaredError)
Var df_test = lrModel.transform(test).select("feature","crash", "prediction")
df_test = df_test.withColumn("crash",col("crash").cast("Double"))
val rddx = df_test.select("prediction").rdd.map(_.getDouble(0))
val rddy = df_test.select("crash").rdd.map(_.getDouble(0))
val correlation: Double = Statistics.corr(rddx, rddy, "spearman")

var df3 = df.select("temp","crash","time")
df3 = df3.withColumn("temp",col("temp").cast("Double"))
df3 = df3.withColumn("crash",col("crash").cast("Double"))
df3 = df3.withColumn("time",col("time").cast("Double"))
var assembler = new VectorAssembler().setInputCols(Array("temp","time")).setOutputCol("feature")
var lr = new LinearRegression()
lr = lr.setFeaturesCol("feature")
lr = lr.setLabelCol("crash")
lr.setRegParam(0.3)
var lrModel = lr.fit(train)
val trainingSummary = lrModel.summary
print(trainingSummary.r2)
print(trainingSummary.rootMeanSquaredError)
var df_test = lrModel.transform(test).select("feature","crash", "prediction")
df_test = df_test.withColumn("crash",col("crash").cast("Double"))
val rddx = df_test.select("prediction").rdd.map(_.getDouble(0))
val rddy = df_test.select("crash").rdd.map(_.getDouble(0))
import org.apache.spark.mllib.stat.Statistics
val correlation: Double = Statistics.corr(rddx, rddy, "spearman")

